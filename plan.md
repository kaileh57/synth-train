# Critical Fixes Implementation Plan

## Overview
This plan contains all fixes needed to make the synthetic data LLM training codebase fully functional. The codebase is well-designed but has missing implementations and some technical issues that need resolution.

## üîß CRITICAL FIXES REQUIRED

### 1. CREATE MISSING EXPERIMENT IMPLEMENTATIONS

#### 1.1 Create `experiments/exp3-dataset-mixing/data_mixer.py`
```python
#!/usr/bin/env python3
"""
Dataset mixing implementation for Experiment 3
Creates optimal combinations of different synthetic datasets
"""

import os
import json
import random
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Union
from collections import Counter
import logging

import numpy as np
import pandas as pd
from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets
from transformers import AutoTokenizer
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DatasetMixer:
    """Mix different synthetic datasets with specified ratios"""
    
    def __init__(
        self,
        cache_dir: str = "./cache",
        save_dir: str = "./data/mixed_datasets",
        seed: int = 42
    ):
        self.cache_dir = Path(cache_dir)
        self.save_dir = Path(save_dir)
        self.seed = seed
        
        # Create directories
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # Set seed for reproducibility
        random.seed(seed)
        np.random.seed(seed)
        
        # Initialize tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained("gpt2")
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Dataset configurations
        self.dataset_configs = {
            'openhermes': {
                'name': 'teknium/OpenHermes-2.5',
                'split': 'train',
                'format_func': self._format_openhermes,
                'description': 'High-quality instruction-following data from GPT-4'
            },
            'cosmopedia': {
                'name': 'HuggingFaceTB/cosmopedia',
                'split': 'train',
                'format_func': self._format_cosmopedia,
                'description': 'Educational content generated by Mixtral-8x7B',
                'config': 'auto_math_text'  # Use smaller subset
            },
            'magpie': {
                'name': 'Magpie-Align/MagpieLM-Pro-300K-v0.1',
                'split': 'train',
                'format_func': self._format_magpie,
                'description': 'Multi-turn conversations from Llama-3.1-70B'
            },
            'fineweb': {
                'name': 'HuggingFaceFW/fineweb-edu',
                'split': 'train',
                'format_func': self._format_fineweb,
                'description': 'High-quality educational web content'
            }
        }
        
        self.loaded_datasets = {}
        self.dataset_stats = {}
    
    def load_dataset_sample(self, dataset_name: str, max_samples: int) -> List[Dict]:
        """Load a sample from a dataset"""
        
        if dataset_name not in self.dataset_configs:
            raise ValueError(f"Unknown dataset: {dataset_name}")
        
        config = self.dataset_configs[dataset_name]
        logger.info(f"Loading {dataset_name} ({config['description']})")
        
        # Load dataset with streaming for memory efficiency
        try:
            if 'config' in config:
                dataset = load_dataset(
                    config['name'],
                    config['config'],
                    split=config['split'],
                    streaming=True,
                    cache_dir=self.cache_dir
                )
            else:
                dataset = load_dataset(
                    config['name'],
                    split=config['split'],
                    streaming=True,
                    cache_dir=self.cache_dir
                )
            
            # Take samples
            samples = []
            for i, sample in enumerate(dataset):
                if i >= max_samples:
                    break
                
                # Format sample
                formatted = config['format_func'](sample)
                if formatted:  # Skip if formatting failed
                    formatted['source'] = dataset_name
                    formatted['source_index'] = i
                    samples.append(formatted)
            
            logger.info(f"Loaded {len(samples)} samples from {dataset_name}")
            return samples
            
        except Exception as e:
            logger.error(f"Failed to load {dataset_name}: {e}")
            return []
    
    def _format_openhermes(self, sample: Dict) -> Optional[Dict]:
        """Format OpenHermes sample"""
        try:
            if "conversations" in sample:
                # Multi-turn conversation format
                text = ""
                for turn in sample["conversations"]:
                    role = turn.get("from", "")
                    content = turn.get("value", "")
                    if role == "system":
                        text += f"System: {content}\n\n"
                    elif role == "human":
                        text += f"### Instruction:\n{content}\n\n"
                    elif role == "gpt":
                        text += f"### Response:\n{content}\n\n"
                
                return {
                    "text": text.strip(),
                    "type": "instruction",
                    "length": len(text.split())
                }
            else:
                # Simple format
                instruction = sample.get("instruction", "")
                response = sample.get("output", sample.get("response", ""))
                
                if not instruction or not response:
                    return None
                
                text = f"### Instruction:\n{instruction}\n\n### Response:\n{response}"
                return {
                    "text": text,
                    "type": "instruction",
                    "length": len(text.split())
                }
        except Exception:
            return None
    
    def _format_cosmopedia(self, sample: Dict) -> Optional[Dict]:
        """Format Cosmopedia sample"""
        try:
            text = sample.get("text", "")
            if not text or len(text) < 100:  # Skip very short texts
                return None
            
            # Limit length for memory efficiency
            words = text.split()
            if len(words) > 1000:  # Limit to ~1000 words
                text = " ".join(words[:1000])
            
            return {
                "text": text,
                "type": "educational",
                "length": len(text.split())
            }
        except Exception:
            return None
    
    def _format_magpie(self, sample: Dict) -> Optional[Dict]:
        """Format Magpie sample"""
        try:
            conversations = sample.get("conversations", [])
            if not conversations:
                return None
            
            text = ""
            for turn in conversations:
                role = turn.get("role", "")
                content = turn.get("content", "")
                
                if role == "user":
                    text += f"User: {content}\n\n"
                elif role == "assistant":
                    text += f"Assistant: {content}\n\n"
            
            if not text:
                return None
            
            return {
                "text": text.strip(),
                "type": "conversation",
                "length": len(text.split())
            }
        except Exception:
            return None
    
    def _format_fineweb(self, sample: Dict) -> Optional[Dict]:
        """Format FineWeb sample"""
        try:
            text = sample.get("text", "")
            if not text or len(text) < 200:  # Skip very short texts
                return None
            
            # Limit length for memory efficiency
            words = text.split()
            if len(words) > 800:  # Limit to ~800 words
                text = " ".join(words[:800])
            
            return {
                "text": text,
                "type": "web_educational",
                "length": len(text.split())
            }
        except Exception:
            return None
    
    def create_mixed_dataset(
        self,
        mixing_ratios: Dict[str, float],
        total_samples: int = 100000,
        strategy_name: str = "custom_mix"
    ) -> Dataset:
        """Create a mixed dataset according to specified ratios"""
        
        # Validate ratios
        total_ratio = sum(mixing_ratios.values())
        if abs(total_ratio - 1.0) > 0.01:
            logger.warning(f"Mixing ratios sum to {total_ratio}, normalizing...")
            mixing_ratios = {k: v/total_ratio for k, v in mixing_ratios.items()}
        
        # Calculate samples per dataset
        samples_per_dataset = {}
        for dataset_name, ratio in mixing_ratios.items():
            samples_per_dataset[dataset_name] = int(ratio * total_samples)
        
        logger.info(f"Creating {strategy_name} with {total_samples} total samples:")
        for dataset_name, count in samples_per_dataset.items():
            logger.info(f"  {dataset_name}: {count} samples ({mixing_ratios[dataset_name]*100:.1f}%)")
        
        # Load samples from each dataset
        all_samples = []
        dataset_contributions = {}
        
        for dataset_name, num_samples in samples_per_dataset.items():
            if num_samples == 0:
                continue
                
            samples = self.load_dataset_sample(dataset_name, num_samples)
            all_samples.extend(samples)
            dataset_contributions[dataset_name] = len(samples)
            
            logger.info(f"Added {len(samples)} samples from {dataset_name}")
        
        # Shuffle for good mixing
        random.shuffle(all_samples)
        
        # Convert to HuggingFace Dataset
        dataset = Dataset.from_list(all_samples)
        
        # Add metadata
        metadata = {
            "strategy_name": strategy_name,
            "mixing_ratios": mixing_ratios,
            "total_samples": len(all_samples),
            "dataset_contributions": dataset_contributions,
            "creation_time": pd.Timestamp.now().isoformat(),
            "seed": self.seed
        }
        
        return dataset, metadata
    
    def save_mixed_dataset(
        self,
        dataset: Dataset,
        metadata: Dict,
        strategy_name: str
    ):
        """Save mixed dataset and associated metadata"""
        
        # Create directory for this strategy
        strategy_dir = self.save_dir / strategy_name
        strategy_dir.mkdir(parents=True, exist_ok=True)
        
        # Split into train/validation
        dataset_split = dataset.train_test_split(test_size=0.05, seed=self.seed)
        
        # Save dataset
        dataset_split.save_to_disk(str(strategy_dir))
        
        # Save metadata
        with open(strategy_dir / "metadata.json", "w") as f:
            json.dump(metadata, f, indent=2)
        
        logger.info(f"Saved {strategy_name} to {strategy_dir}")
        
        return strategy_dir


# Predefined mixing strategies
MIXING_STRATEGIES = {
    "equal_mix": {
        "openhermes": 0.25,
        "cosmopedia": 0.25,
        "magpie": 0.25,
        "fineweb": 0.25
    },
    "instruction_heavy": {
        "openhermes": 0.40,
        "cosmopedia": 0.20,
        "magpie": 0.30,
        "fineweb": 0.10
    },
    "knowledge_heavy": {
        "openhermes": 0.20,
        "cosmopedia": 0.40,
        "magpie": 0.10,
        "fineweb": 0.30
    },
    "conversation_heavy": {
        "openhermes": 0.20,
        "cosmopedia": 0.10,
        "magpie": 0.50,
        "fineweb": 0.20
    },
    "quality_weighted": {
        "openhermes": 0.35,  # Highest quality
        "cosmopedia": 0.25,
        "magpie": 0.30,
        "fineweb": 0.10
    },
    "capability_balanced": {
        "openhermes": 0.30,
        "cosmopedia": 0.25,
        "magpie": 0.25,
        "fineweb": 0.20
    }
}


def main():
    parser = argparse.ArgumentParser(description="Create mixed datasets for Experiment 3")
    parser.add_argument("--strategy", type=str, choices=list(MIXING_STRATEGIES.keys()) + ["all"],
                        default="all", help="Mixing strategy to create")
    parser.add_argument("--total_samples", type=int, default=100000,
                        help="Total number of samples in mixed dataset")
    parser.add_argument("--output_dir", type=str, default="./data/mixed_datasets",
                        help="Output directory for mixed datasets")
    parser.add_argument("--cache_dir", type=str, default="./cache",
                        help="Cache directory for downloads")
    
    args = parser.parse_args()
    
    # Initialize mixer
    mixer = DatasetMixer(
        cache_dir=args.cache_dir,
        save_dir=args.output_dir
    )
    
    # Determine which strategies to run
    if args.strategy == "all":
        strategies_to_run = MIXING_STRATEGIES
    else:
        strategies_to_run = {args.strategy: MIXING_STRATEGIES[args.strategy]}
    
    # Create each strategy
    for strategy_name, mixing_ratios in strategies_to_run.items():
        logger.info(f"\nCreating strategy: {strategy_name}")
        
        # Create mixed dataset
        dataset, metadata = mixer.create_mixed_dataset(
            mixing_ratios,
            total_samples=args.total_samples,
            strategy_name=strategy_name
        )
        
        # Save everything
        mixer.save_mixed_dataset(dataset, metadata, strategy_name)
        
        logger.info(f"Strategy {strategy_name} complete!")
    
    logger.info(f"\nAll mixing strategies complete! Results saved to {args.output_dir}")


if __name__ == "__main__":
    main()
```

#### 1.2 Create `experiments/exp3-dataset-mixing/train_mixed_models.py`
```python
#!/usr/bin/env python3
"""
Training script for mixed datasets in Experiment 3
"""

import os
import sys
import argparse
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from experiments.exp1_pure_synthetic.model_configs import create_model_config
from experiments.exp1_pure_synthetic.train_pure_synthetic import main as train_main

def main():
    parser = argparse.ArgumentParser(description="Train models on mixed datasets")
    parser.add_argument("--strategy", type=str, required=True, help="Mixing strategy name")
    parser.add_argument("--model_size", type=str, default="500M", help="Model size")
    parser.add_argument("--dataset_dir", type=str, default="./data/mixed_datasets", help="Mixed datasets directory")
    
    args = parser.parse_args()
    
    # Set dataset path
    dataset_path = f"{args.dataset_dir}/{args.strategy}"
    
    # Run training with mixed dataset
    train_args = [
        "--dataset_path", dataset_path,
        "--model_size", args.model_size,
        "--output_dir", f"./models/exp3-{args.strategy}",
        "--run_name", f"exp3-{args.strategy}-{args.model_size}",
        "--fp16",
        "--gradient_checkpointing"
    ]
    
    # Call training function
    sys.argv = ["train_mixed_models.py"] + train_args
    train_main()

if __name__ == "__main__":
    main()
```

#### 1.3 Create `experiments/exp3-dataset-mixing/README.md`
```markdown
# Experiment 3: Dataset Mixing

## Overview
This experiment tests different mixing strategies for combining synthetic datasets to maximize model capabilities.

## Quick Start

1. **Create mixed datasets:**
   ```bash
   python data_mixer.py --strategy all --total_samples 100000
   ```

2. **Train models on mixed datasets:**
   ```bash
   python train_mixed_models.py --strategy equal_mix --model_size 500M
   python train_mixed_models.py --strategy instruction_heavy --model_size 500M
   # etc.
   ```

3. **Evaluate results:**
   ```bash
   python analyze_mixing_results.py
   ```

## Mixing Strategies

- **equal_mix**: 25% each dataset
- **instruction_heavy**: 40% OpenHermes, 30% Magpie, 20% Cosmopedia, 10% FineWeb
- **knowledge_heavy**: 40% Cosmopedia, 30% FineWeb, 20% OpenHermes, 10% Magpie
- **conversation_heavy**: 50% Magpie, 20% each others
- **quality_weighted**: Based on quality scores from Experiment 2
- **capability_balanced**: Optimized for diverse capabilities
```

### 2. FIX GPU UTILIZATION CODE

#### 2.1 Fix in `experiments/exp1-pure-synthetic/train_pure_synthetic.py`

**Replace lines 155-165:**
```python
# Handle asymmetric multi-GPU setup
if torch.cuda.device_count() > 1:
    # This device map is optimized for an A6000 + A4500 setup with a 24-layer model.
    # It places more layers on the more powerful GPU (assumed to be device 0).
    if model_config.n_layer == 24:
        logger.info("Applying asymmetric device map for 2-GPU setup.")
        device_map = {
            0: list(range(12)),  # First 12 layers on GPU 0 (e.g., A6000)
            1: list(range(12, 24)), # Last 12 layers on GPU 1 (e.g., A4500)
        }
        model.parallelize(device_map)
    else:
        logger.info("Using default DataParallel for multi-GPU setup.")
```

**With:**
```python
# Handle asymmetric multi-GPU setup
if torch.cuda.device_count() > 1:
    logger.info(f"Detected {torch.cuda.device_count()} GPUs")
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        logger.info(f"  GPU {i}: {props.name} ({props.total_memory/1e9:.1f}GB)")
    
    # Use DataParallel for model parallelism
    # This works better than manual device mapping for training
    model = torch.nn.DataParallel(model)
    logger.info("Using DataParallel for multi-GPU training")
else:
    logger.info("Using single GPU training")
```

### 3. FIX IMPORT STRUCTURE

#### 3.1 Create `utils/__init__.py`
```python
# Empty file to make utils a package
```

#### 3.2 Create `utils/imports.py`
```python
"""
Centralized import utilities to avoid path issues
"""

import os
import sys
from pathlib import Path

def add_project_root():
    """Add project root to Python path"""
    # Find project root (where budget.py is located)
    current_dir = Path(__file__).parent
    project_root = current_dir.parent
    
    # Look for budget.py as marker of project root
    while project_root != project_root.parent:
        if (project_root / "budget.py").exists():
            break
        project_root = project_root.parent
    
    # Add to path if not already there
    project_root_str = str(project_root)
    if project_root_str not in sys.path:
        sys.path.insert(0, project_root_str)
    
    return project_root

def safe_import_budget():
    """Safely import budget tracker"""
    try:
        add_project_root()
        from budget import BudgetTracker
        return BudgetTracker
    except ImportError:
        # Return a dummy class if budget.py not found
        class DummyBudgetTracker:
            def record_expense(self, hours, description):
                print(f"Budget tracking unavailable: {description} - {hours} hours")
            def get_summary(self):
                return "Budget tracking unavailable"
        return DummyBudgetTracker
```

#### 3.3 Update All Training Scripts to Use New Import System

**Fix needed in:**
- `experiments/exp1-pure-synthetic/train_pure_synthetic.py`
- `experiments/exp2-quality-vs-quantity/train_quality_comparison.py`

**Replace this pattern:**
```python
# Add project root to path to allow importing budget
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.insert(0, project_root)
```

**With this:**
```python
# Import budget tracking safely
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
try:
    from utils.imports import safe_import_budget
    BudgetTracker = safe_import_budget()
except ImportError:
    # Fallback if utils not available
    class BudgetTracker:
        def record_expense(self, hours, description):
            print(f"Budget: {description} - {hours} hours")
        def get_summary(self):
            return "Budget tracking unavailable"
```

### 4. FIX MISSING DEPENDENCIES

#### 4.1 Update `requirements.txt` - Add Missing Packages
```
# Add these lines to requirements.txt:
scipy==1.11.4
py-cpuinfo==9.0.0
packaging==23.2
```

#### 4.2 Update `setup.sh` - Add Missing Installations
```bash
# Add these lines to setup.sh after line 180 (after installing evaluation libraries):

# Install missing dependencies for evaluation framework
print_status "Installing additional evaluation dependencies..."
$PYTHON_CMD -m pip install packaging==23.2
$PYTHON_CMD -m pip install py-cpuinfo==9.0.0

# Install missing dependencies for mixing experiments  
print_status "Installing dataset mixing dependencies..."
$PYTHON_CMD -m pip install jaxtyping==0.2.24
```

### 5. CREATE VALIDATION SCRIPT

#### 5.1 Create `test_all_fixes.py` in project root
```python
#!/usr/bin/env python3
"""
Test script to validate all fixes are working
"""

import os
import sys
from pathlib import Path
import importlib.util

def test_imports():
    """Test that all imports work correctly"""
    print("Testing imports...")
    
    # Test utils imports
    try:
        from utils.imports import add_project_root, safe_import_budget
        print("‚úì utils.imports working")
    except ImportError as e:
        print(f"‚úó utils.imports failed: {e}")
        return False
    
    # Test budget import
    try:
        BudgetTracker = safe_import_budget()
        tracker = BudgetTracker()
        print("‚úì Budget tracking working")
    except Exception as e:
        print(f"‚úó Budget tracking failed: {e}")
    
    return True

def test_experiment_files():
    """Test that all experiment files exist"""
    print("Testing experiment files...")
    
    required_files = [
        "experiments/exp3-dataset-mixing/data_mixer.py",
        "experiments/exp4-zero-cost-eval/evaluator.py",
        "utils/imports.py"
    ]
    
    all_good = True
    for file_path in required_files:
        if Path(file_path).exists():
            print(f"‚úì {file_path}")
        else:
            print(f"‚úó {file_path} missing")
            all_good = False
    
    return all_good

def test_module_imports():
    """Test that modules can be imported"""
    print("Testing module imports...")
    
    # Test exp3 import
    try:
        sys.path.append("experiments/exp3-dataset-mixing")
        import data_mixer
        print("‚úì exp3 data_mixer imports correctly")
    except ImportError as e:
        print(f"‚úó exp3 data_mixer import failed: {e}")
        return False
    
    # Test exp4 import
    try:
        sys.path.append("experiments/exp4-zero-cost-eval")
        import evaluator
        print("‚úì exp4 evaluator imports correctly")
    except ImportError as e:
        print(f"‚úó exp4 evaluator import failed: {e}")
        return False
    
    return True

def main():
    print("Running validation tests for all fixes...")
    print("=" * 50)
    
    tests = [
        ("Import structure", test_imports),
        ("File existence", test_experiment_files),
        ("Module imports", test_module_imports)
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\n{test_name}:")
        result = test_func()
        results.append((test_name, result))
    
    print("\n" + "=" * 50)
    print("Test Results:")
    
    all_passed = True
    for test_name, result in results:
        status = "PASS" if result else "FAIL"
        print(f"{test_name}: {status}")
        if not result:
            all_passed = False
    
    if all_passed:
        print("\n‚úÖ All tests passed! Ready to run experiments.")
    else:
        print("\n‚ùå Some tests failed. Please fix the issues above.")
    
    return 0 if all_passed else 1

if __name__ == "__main__":
    sys.exit(main())
```

## üîß TESTING & VALIDATION

### 6. IMPLEMENTATION CHECKLIST

#### Phase 1: Core Fixes (30 minutes)
- [ ] Create `experiments/exp3-dataset-mixing/` directory
- [ ] Add `data_mixer.py` to exp3 directory
- [ ] Add `train_mixed_models.py` to exp3 directory  
- [ ] Add `README.md` to exp3 directory
- [ ] Create `utils/` directory
- [ ] Add `__init__.py` and `imports.py` to utils directory

#### Phase 2: Import Fixes (15 minutes)
- [ ] Update `experiments/exp1-pure-synthetic/train_pure_synthetic.py` imports
- [ ] Update `experiments/exp2-quality-vs-quantity/train_quality_comparison.py` imports
- [ ] Fix GPU utilization code in exp1 training script

#### Phase 3: Dependencies (10 minutes)
- [ ] Add missing packages to `requirements.txt`
- [ ] Update `setup.sh` with additional dependencies
- [ ] Create `test_all_fixes.py` validation script

#### Phase 4: Testing (15 minutes)
- [ ] Run `python test_all_fixes.py`
- [ ] Fix any remaining import issues
- [ ] Test basic functionality: `cd experiments/exp4-zero-cost-eval && python evaluator.py`

## üéØ EXPECTED RESULTS AFTER FIXES

1. **All experiments functional** - No more missing implementations
2. **Robust imports** - No more fragile path manipulation  
3. **GPU utilization working** - Proper multi-GPU support for A6000+A4500
4. **Zero-cost evaluation ready** - Complete framework implementation
5. **Dataset mixing ready** - All 6 strategies implementable

## ‚ö° IMMEDIATE NEXT STEPS AFTER FIXES

1. **Validate setup:** `python test_all_fixes.py`
2. **Test evaluation:** `cd experiments/exp4-zero-cost-eval && python evaluator.py`
3. **Test mixing:** `cd experiments/exp3-dataset-mixing && python data_mixer.py --strategy equal_mix --total_samples 1000`
4. **Run first real experiment:** Start with Experiment 1 on small sample

The codebase will be production-ready after these fixes!