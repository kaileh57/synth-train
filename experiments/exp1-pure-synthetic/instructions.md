# Experiment 1: Pure Synthetic Excellence - Instructions

## Overview
This experiment tests whether high-quality synthetic data (GPT-4 generated) alone can train capable small-scale LLMs, establishing a baseline for synthetic data effectiveness.

## Objectives
1. Train 500M and 1B parameter models on OpenHermes-2.5 dataset
2. Establish baseline performance metrics for pure synthetic training
3. Validate that synthetic-only training is viable for small models
4. Compare against published results for similar-sized models

## Dataset: OpenHermes-2.5
- **Source**: teknium/OpenHermes-2.5 on HuggingFace
- **Size**: ~1M high-quality instruction-response pairs
- **Generated by**: Primarily GPT-4
- **Format**: Conversations with system prompts, user queries, and assistant responses

## Implementation Steps

### Step 1: Environment Setup
```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers datasets accelerate wandb
pip install numpy pandas matplotlib seaborn tqdm
```

### Step 2: Data Preparation
1. **Download Dataset**
   ```python
   from datasets import load_dataset
   
   # Load OpenHermes-2.5
   dataset = load_dataset("teknium/OpenHermes-2.5", split="train")
   
   # Sample if needed for initial tests
   dataset_sample = dataset.shuffle(seed=42).select(range(100000))
   ```

2. **Data Processing**
   - Convert to instruction-response format
   - Tokenize with appropriate model tokenizer
   - Create train/validation splits (95/5)
   - Save processed data for reproducibility

### Step 3: Model Configuration

#### 500M Model Config
```python
config_500M = {
    "hidden_size": 1024,
    "num_hidden_layers": 24,
    "num_attention_heads": 16,
    "intermediate_size": 4096,
    "max_position_embeddings": 2048,
    "vocab_size": 50257,
    "n_positions": 2048,
    "n_embd": 1024,
    "n_layer": 24,
    "n_head": 16
}
```

#### 1B Model Config
```python
config_1B = {
    "hidden_size": 1536,
    "num_hidden_layers": 24,
    "num_attention_heads": 24,
    "intermediate_size": 6144,
    "max_position_embeddings": 2048,
    "vocab_size": 50257,
    "n_positions": 2048,
    "n_embd": 1536,
    "n_layer": 24,
    "n_head": 24
}
```

### Step 4: Training Setup

#### Training Arguments
```python
training_args = TrainingArguments(
    output_dir="./models/exp1-500M-pure-synthetic",
    num_train_epochs=1,  # Adjust based on budget
    per_device_train_batch_size=4,
    per_device_eval_batch_size=8,
    gradient_accumulation_steps=8,
    warmup_steps=1000,
    learning_rate=2e-4,
    weight_decay=0.01,
    logging_steps=100,
    save_steps=5000,
    eval_steps=1000,
    evaluation_strategy="steps",
    save_total_limit=3,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    fp16=True,
    gradient_checkpointing=True,
    dataloader_num_workers=4,
    report_to="wandb",
    run_name="exp1-500M-pure-synthetic",
)
```

#### Multi-GPU Configuration
```python
# For distributed training across 8 GPUs
from accelerate import Accelerator

accelerator = Accelerator(
    mixed_precision='fp16',
    gradient_accumulation_steps=8,
)
```

### Step 5: Training Script Structure
```python
# train_pure_synthetic.py

import torch
from transformers import (
    GPT2Config, GPT2LMHeadModel, GPT2Tokenizer,
    Trainer, TrainingArguments, DataCollatorForLanguageModeling
)
from datasets import load_dataset
import wandb

def main():
    # 1. Initialize wandb
    wandb.init(project="synth-train-exp1", name="pure-synthetic-500M")
    
    # 2. Load and prepare data
    dataset = prepare_dataset()
    
    # 3. Initialize model and tokenizer
    model, tokenizer = initialize_model(config_500M)
    
    # 4. Setup training
    trainer = setup_trainer(model, tokenizer, dataset, training_args)
    
    # 5. Train
    trainer.train()
    
    # 6. Evaluate
    results = evaluate_model(model, tokenizer)
    
    # 7. Save everything
    save_results(model, results)

if __name__ == "__main__":
    main()
```

### Step 6: Evaluation Metrics

1. **Perplexity** on held-out validation set
2. **Generation Quality** 
   - Coherence of generated text
   - Instruction-following capability
   - Diversity metrics
3. **Zero-shot Task Performance**
   - Simple Q&A
   - Text completion
   - Basic reasoning

### Step 7: Expected Outputs

1. **Checkpoints**: `models/exp1-500M/` and `models/exp1-1B/`
2. **Metrics Log**: `results/exp1-metrics.json`
3. **Generation Samples**: `results/exp1-generations.txt`
4. **Training Curves**: W&B dashboard or local plots

## Budget Planning

- **500M Model**: ~1 GPU-hour (single GPU sufficient)
- **1B Model**: ~1 GPU-hour (use 4-8 GPUs for speed)
- **Total**: 2 GPU-hours

## Success Criteria

1. **Perplexity** < 20 on validation set
2. **Coherent generation** for 90%+ of prompts
3. **Training stability** (no loss spikes or NaNs)
4. **Reproducible results** across random seeds

## Common Issues & Solutions

1. **OOM Errors**
   - Reduce batch size
   - Enable gradient checkpointing
   - Use mixed precision (fp16)

2. **Slow Training**
   - Check data loading pipeline
   - Use more GPUs with data parallelism
   - Optimize tokenization with caching

3. **Poor Quality**
   - Check data preprocessing
   - Adjust learning rate
   - Increase warmup steps

## Validation Steps

Before full training:
1. Train for 100 steps with small data sample
2. Verify loss decreases
3. Check generation quality early
4. Monitor GPU memory usage

## Key Code Files to Create

1. `data_preparation.py` - Dataset loading and processing
2. `model_configs.py` - Model architecture definitions
3. `train_pure_synthetic.py` - Main training script
4. `evaluate.py` - Evaluation utilities
5. `generate_samples.py` - Generation and analysis

## Next Steps After Completion

1. Analyze results and compare to baselines
2. Identify strengths/weaknesses of pure synthetic approach
3. Use findings to inform Experiment 2 (quality filtering)
4. Document all hyperparameters and results

## References

- OpenHermes-2.5 Dataset: https://huggingface.co/datasets/teknium/OpenHermes-2.5
- GPT-2 Paper: "Language Models are Unsupervised Multitask Learners"
- Training tips: https://huggingface.co/docs/transformers/training 